\documentclass{sig-alternate}
\usepackage{textcomp}

\begin{document}

\title{Paxos Made Visual}

\numberofauthors{2} 

\author{
\alignauthor
Zachary Drudi  \email{zdrudi@cs.ubc.ca}
\alignauthor
Jijie Wei		\email{weijijie@cs.ubc.ca}
}

%\date{30 July 1999}

\maketitle

\begin{abstract}
We tried to implement a variant of the Multi-Paxos protocol. 
\end{abstract}

\section{Introduction}
Distributed systems which use a server-client communication pattern can be thought of as a series of updates on a state machine. The server implements a state machine, and it updates its interior state in response to client requests, responding with the new state. 

The Paxos protocol is used to extend this simple framework with fault tolerance. Instead of a single server node, we now have $N$, each one implementing the state machine. The protocol ensures that each node has the same state as all others at all times, even in presence of multiple clients sending requests to the system simultaneously. Furthermore, as long as a majority of the nodes remain operational, the system will still respond to client requests.

Throughout this term, we have seen many distributed systems which use a designated master node for coordination \cite{dean2008mapreduce, ghemawat_google_2003}. While a single master node is simple and has low communication overhead and latency, the system may be crippled if the master node fails. Paxos is frequently used \cite{thekkath_frangipani:_1997, lee_petal:_1996, corbett_spanner:_2012, calder_windows_2011} to provide crash tolerance to the master node, replacing the single master node with a network of replicated masters. Provided that a majority of them always remain operational, the system will remain available.

Given the frequent mention of Paxos as a subservice within larger distributed systems, we were motivated to learn more about it. The core idea of the protocol can be explained in a just a few pages of prose \cite{lamport_paxos_2001}, so it seemed reasonable to attempt a Paxos implementation as a course project. Finally, its reputation as being difficult to implement gave it the feel of a suitable challenge for a course project, especially for two non-systems students looking to prove themselves.

Our implementation is written in Scala, and uses remote actors to model the processes of Paxos. We also built a simple Swing application to give a visual front-end to the protocol, using colours to represent the state of the different nodes. It's easy to see at a glance that all the machines are in sync, and after a crash we can see the crashed machine fail to update as the others remain in lockstep.

\section{The Protocol}

The Paxos algorithm is described in many papers \cite{lamport_part-time_1998, lamport_paxos_2001, lampson_abcds_2001, van_renesse_paxos_2011} and each one presents a slightly different variant of the protocol using different terminology. As we used \cite{van_renesse_paxos_2011} as a starting point for our implementation, we describe the protocol using their terminology.

\subsection{Single-Round Paxos - The Synod Protocol}
The core protocol in Paxos is the Synod protocol. The Synod protocol achieves consensus on a single value among a group of processes. There are two types of processes in the Synod protocol, Leaders and Acceptors. Leaders propose new ballots, which consist of a ballot number and the id of the leader proposing it, and Acceptors decide whether to adopt, accept, or reject ballots they receive from Leaders. We assume there is a total ordering on ballots; this can be achieved by lexicographic comparison of ballots.

Acceptors are purely reactive, and wait to receive messages from leaders. They maintain two private variables, $highest\_b$, the highest ballot they've received so far, and $accepted$, the set of proposals, which are ballot, value pairs, they've accepted. They respond to two types of messages:
\begin{enumerate}
	\item Phase 1 message \textlangle \textbf{p1a}, ballot\textrangle: The receiving Acceptor compares their private highest ballot with the ballot in the message. If the received ballot is greater, they update their highest ballot variable to the received ballot, and we say the Acceptor adopts the ballot. In any case, they respond to the sending Leader with the message \textlangle \textbf{p1b}, $highest\_b$, $accepted$\textrangle.
	
	\item Phase 2 message \textlangle \textbf{p2a}, $b$, $value$\textrangle:
	If the Acceptor's internal $highest_b$ is less than $b$, it updates $highest_b$. Also, in this case it accepts the ballot by adding \textlangle$b$, $value$\textrangle  to $accepted$. In any event, the Acceptor responds to the sending Leader with the message \textlangle \textbf{p2b}, $highest\_b$\textrangle.
\end{enumerate}

Leaders are responsible for proposing ballots to Acceptors. 

\begin{enumerate}
	\item Phase 1: When a Leader decides to initiate a ballot, it chooses a ballot number $b$ and some set of Acceptors which contains a majority. It sends the message \textlangle \textbf{p1a}, $b$\textrangle to each Acceptor in this set, and waits to receive their responses. If it receives a message\textlangle \textbf{p1b}, $b'$, $v$ \textrangle with $b < b'$, then it knows there is another Leader which has proposed a higher ballot, and so it abandons this ballot and initiates a new ballot with a higher ballot number.
	The other possibility is that $b' = b$, in which case the Leader can conclude that the Acceptor who sent this message adopted the ballot $b$. If the Leader receives such messages from a majority of Acceptors, then it proceeds to phase 2 of the Leader protocol.
	
	\item Phase 2: The Leader chooses the value $v$ associated with the highest ballot among all responses to its Phase 1 message, and sends the message \textlangle \textbf{p2a}, $b$, $v$ \textrangle to all responding Acceptors from Phase 1. It then waits for responses.
	As in Phase 1, if it receives a \textlangle \textbf{p2b}, $b'$ \textrangle response from an Acceptor with a $b < b'$, then the Leader concludes it has been pre-empted by a competing Leader, and restarts Phase 1 with a higher ballot number.
	On the other hand, if it receives responses from a majority of Acceptors with the same ballot number $b$, then the Leader concludes that its proposed value has been accepted by a majority of the Acceptors. In this case, we say the value $v$ is \textit{chosen}.
\end{enumerate}

The guarantee provided by the Synod protocol is that only one value can ever be chosen. Any future ballot will simply result in a majority of Acceptors re-accepting the previously chosen value. As an informal argument, suppose the value $v$ is chosen. By definition, a majority of Acceptors have accepted the value $v$. Consider the first Leader $L$ which proposes a ballot after $v$ is chosen. Now, since $L$ waits to hear responses from a majority of Acceptors in Phase 1 before it chooses a value to propose, it will receive a response \textlangle \textbf{p1b}, $b$, $v$\textrangle from some Acceptor that participated in the successful ballot to choose $v$. The associated ballot number $b$ must be higher than any other ballot number $L$ receives in \textbf{p1b} messages, since the fact that $v$ was chosen means $b$ beat all other ballots in existence when it was proposed, and $L$ is the first leader to conduct Phase 1 after $v$ was chosen. Thus $L$ will propose $v$ in Phase 2.

Note that as stated, the Synod protocol is susceptible to livelocks.  .... XXX todo

\subsection{Multi-Round Paxos}

While satisfying from a theoretical perspective, agreeing on a single value once is not terribly useful. Paxos is extended to support a replicated state machine by performing the Synod protocol multiple times in rounds.

There are additional processes in this model, Replicas and Clients.

Replicas implement the distributed state machine. Each Replica maintains a local copy of the state of the distributed state machine, as well as the ordered list of operations performed to transition from the initial state to the currently maintained state. This state is updated as a result of commands from Clients. To ensure that all Replicas maintain the same state at the same time, even in the presence of multiple Clients issuing commands concurrently, Replicas use the Synod protocol to decide which command to perform in which order.

Upon receiving a new command from a Client, a Replica begins a new instance of the Synod protocol by starting a new Leader and a new Acceptor, and forwarding the operation the client requested as well as the next available slot number it can be performed in as the value to propose to the Leader. The Leaders and Acceptors then perform the Synod protocol to achieve consensus on a value, and the Leader broadcasts the accepted value to all Replicas, which update their internal state according to the client operation chosen, and respond to the Client which sent the chosen operation to notify them that the request was performed.

While conceptually clear, this iterated version of Paxos presents some implementation challenges. Our actual implementation is different, and maintains a single Acceptor, Leader, and Replica process at each node at all times. We followed the implementation strategy given in (XXX cite PMMC) quite closely, and we forward the interested reader to that paper for full details.


\section{The Implementation}

We wrote our implementation using the Scala programming language. Scala is a relatively new language developed in 2003 which runs on the JVM (XXX cite). We chose Scala because it has library support for Actors, supports functional and object-oriented programming, and has a lightweight syntax. 

\subsection{Concurrency Model - Actors}
We used actors to represent each participant in the Paxos protocol. Actors are a model of concurrency first developed in 1973 in (XXX cite paper). The Scala implementation of actors combines a thread of control with an object with internal state and methods, and use message passing to communicate. Conceptually, actors do not share any state with one another. Compared to shared-state models of concurrency such as threads, the actor model is much simpler to reason about many race conditions are avoided and locks are unnecessary. Unfortunately, actors are not a silver bullet and concurrent programming is still difficult. It is quite possible to experience deadlocks and livelocks using the actor model.

\subsection{Failure Model}
What failure model do we support?

\subsection{Design}
Our implementation of Paxos can run on a network of nodes. Each node runs 3 actors concurrently: a Leader, an Acceptor, and a Replica. 

Commanders? Scouts?

what do leaders do...

what do acceptors do...

what do replicas do...
describe here, or in the above?
describe differences between abstract algorithm and our implementation here

\subsection{Leader Contention}

Here we discuss aspects of the protocol that are treated in a cursory manner in the literature, but are crucial to a practical algorithm.

First, the issue of leader contention. In the simple version outlined above, Paxos is highly vulnerable to livelocks: suppose two leaders If two leaders L1 and L2 are trying to complete phase 1 of the algorithm. L1 chooses a ballot number, and sends \textbf{p1a} messages to all acceptors. A little later, L2 chooses a ballot number and sends p1a messages. If L2 has a lower ballot number than L1, an acceptor will send a \textbf{preempted} message to L2. Now L2 will choose a higher ballot number, and broadcast \textbf{p1a} messages to acceptors. After hearing back from a majority of the acceptors, L1 will broadcast \textbf{p2a} messages. If some acceptor received L2's second \textbf{p1a} before L1's \textbf{p2a} message, it will send a \textbf{preempted} message to L1, causing L1 to repeat phase 1. This sequence of steps could repeat indefinitely, leading to livelock.

There are several possible solutions to this. One is an exponential back off strategy, similar to what the TCP protocol uses (XXX reference?). Upon receiving a \textbf{preempted} message, a Leader will wait for some fixed period of time $t$ before restarting phase 1. If it is pre-empted again after restarting phase 1, it will wait for $2t$, and will continue to double the wait period until it completes phase 1 and becomes active. If a livelock situation arises, then because of the doubling timeout period, eventually some Leader will have enough time to complete phase 1 of the protocol and send out all proposals. This solution is simple, and behaves well with faults. Leaders are indefinite to node crashes; indeed this gives them a better chance of a proposal, since there are few competing Leaders active.

Another approach is to use heartbeat messages. When a Leader $L$ receives a \textbf{preempted} message, it starts pinging the other Leader $L'$ who had a higher ballot number. While this Leader responds timely to pings, $L$ remains passive and does not send messages. If $L'$ does not respond to a heartbeat message within a given period, then $L$ will begin phase 1 of the protocol. This is the approach we took in our implementation.

\section{Experiences}
war stories

\subsection{Understanding the Algorithm}

Paxos has a reputation for being difficult to implement. Despite this, many papers describe Paxos quite straightforwardly and simply. However, all presentations we could find simply outline a simple, impractical version of the protocol. Many authors are vague when describing optimizations to keep the memory usage practical and additions to the protocol to solve leader contention issues. While arguably outside the main thrust of the conceptual core of Paxos, these questions are vital to implementers, and it would be extremely helpful if some authors described them with richer detail and attention.

\subsection{Distributed Versus Local}
Our initial prototype of the Synod protocol ran on a single JVM. We abused this environment, treating actors as local objects we could communicate with through method calls and field accesses, rather than strictly performing all communication through message passing.

A related technical issue as we transitioned from a local to a distributed implementation was JVM serializability. In order to send objects in messages to actors at possibly remote JVMs, the runtime must serialize the object into a form which can be reconstituted on the remote JVM. Unfortunately, Actor objects in the Scala Actor library cannot be serialized, and our initial implementation sent actors in messages so the receiver could respond to the sender, and print out an informative debug message using the sender's fields. Altogether, changing the implementation so it could work in a distributed setting with RemoteActors involved a surprising amount of work. 

As we were both new to Scala, we wanted to get a prototype working as quickly as possible without spending too much time studying APIs, so after reading the minimum about the Actor library we set to work. Given how time-consuming the required changes were, I believe this was a mistake. We should have started from the very beginning using APIs which support a distributed setting.

\subsection{Underestimating the Difficulty of Concurrency}
Our initial prototype had one leader, fixed ahead of time. We added multiple leaders relatively late in the timeline of the project, and didn't test the initial multiple leader implementation very much. We only discovered after integrating our Paxos service with the GUI that there were timing bugs with our multiple leader implementation. Unfortunately, this issue was nondeterministic, with some test runs proving successful and others deadlocking. 


\section{Optimizations}

\subsection{Garbage Collection}

\subsection{Messages}

The current model of message passing is extremely inefficient. Clients broadcast their commands to all Replicas, which then broadcast to all Leaders. In our current implementation, this is quadratic in the number of nodes, as each node hosts both a replica and a server. 

Instead of broadcasting, the Client could simply send their request to a random Replica. If the Client hears back from the replica within a given period, then the Client knows the command has entered the system. Otherwise, it will chose another replica. Assuming a majority of Replicas are still operational, the Client will eventually pick a functioning Replica. Of course, it could avoid resending requests to Replicas it knows are unresponsive.

We can take a similar approach for Replicas. Instead of broadcasting to all Leaders, Replicas should only forward their request to their co-located leader. Given our failure model is at the node level, not the actor level, if a Replica is operational then so is its co-located Leader. After receiving the \textbf{request} message, the Leader will spawn Commanders to begin phase 2 if it is active, or forward this message to the leader it is currently pinging if it is inactive.

A final idea to reduce the communication bandwidth is to remove the pinging mechanism altogether in favour of the exponential back off approach. This change could work well with the above optimizations. Under this change, 

hmmm not so sure about this, the leader might be waiting for a long time? bandwidth vs latency tradeoff? what is the max wait time? time for a leader to complete all phases? what about multiple leaders?

\section{Conclusions}
what we achieved, what we failed, etc.


% The following two commands are all you need in the
% initial runs of your .tex file to
% produce the bibliography for the citations in your paper.
\bibliographystyle{abbrv}
\bibliography{paxos}  % sigproc.bib is the name of the Bibliography in this case
% You must have a proper ".bib" file
%  and remember to run:
% latex bibtex latex latex
% to resolve all references
%
% ACM needs 'a single self-contained file'!
%
%APPENDICES are optional
%\balancecolumns
% That's all folks!
\end{document}
