\documentclass{sig-alternate}

\begin{document}

\title{Paxos Made Visual}

\numberofauthors{2} 

\author{
\alignauthor
Zachary Drudi  \email{zdrudi@cs.ubc.ca}
\alignauthor
Jijie Wei		\email{weijijie@cs.ubc.ca}
}

%\date{30 July 1999}

\maketitle

\begin{abstract}
We tried to implement a variant of the Multi-Paxos protocol. 
\end{abstract}

\section{Introduction}
Describe the use of Paxos - replicated state machine
list of papers where it is used?
describe our project - what does it do? what works, what doesn't?


\section{The Protocol}

The Paxos algorithm is described in many papers, and each one presents a slightly different variant of the protocol using different terminology. As we used (XXX cite MP) as a starting point for our implementation, we describe the protocol using their terminology.

\subsection{Single-Round Paxos - The Synod Protocol}
The core protocol in the algorithm 



\subsection{Multiple Rounds}
multiple round paxos...

\section{The Implementation}

We wrote our implementation using the Scala programming language. Scala is a relatively new language developed in 2003 which runs on the JVM (XXX cite). We chose Scala because it has library support for Actors, supports functional and object-oriented programming, and has a lightweight syntax. 

\subsection{Concurrency Model - Actors}
We used actors to represent each participant in the Paxos protocol. Actors are a model of concurrency first developed in 1973 in (XXX cite paper). The Scala implementation of actors combines a thread of control with an object with internal state and methods, and use message passing to communicate. Conceptually, actors do not share any state with one another. Compared to shared-state models of concurrency such as threads, the actor model is much simpler to reason about many race conditions are avoided and locks are unnecessary. Unfortunately, actors are not a silver bullet and concurrent programming is still difficult. It is quite possible to experience deadlocks and livelocks using the actor model.

\subsection{Failure Model}
What failure model do we support?

\subsection{Design}
Our implementation of Paxos can run on a network of nodes. Each node runs 3 actors concurrently: a Leader, an Acceptor, and a Replica. 

Commanders? Scouts?

what do leaders do...

what do acceptors do...

what do replicas do...
describe here, or in the above?
describe differences between abstract algorithm and our implementation here

\subsection{Leader Contention}

Here we discuss aspects of the protocol that are treated in a cursory manner in the literature, but are crucial to a practical algorithm.

First, the issue of leader contention. In the simple version outlined above, Paxos is highly vulnerable to livelocks: suppose two leaders If two leaders L1 and L2 are trying to complete phase 1 of the algorithm. L1 chooses a ballot number, and sends \textbf{p1a} messages to all acceptors. A little later, L2 chooses a ballot number and sends p1a messages. If L2 has a lower ballot number than L1, an acceptor will send a \textbf{preempted} message to L2. Now L2 will choose a higher ballot number, and broadcast \textbf{p1a} messages to acceptors. After hearing back from a majority of the acceptors, L1 will broadcast \textbf{p2a} messages. If some acceptor received L2's second \textbf{p1a} before L1's \textbf{p2a} message, it will send a \textbf{preempted} message to L1, causing L1 to repeat phase 1. This sequence of steps could repeat indefinitely, leading to livelock.

There are several possible solutions to this. One is an exponential back off strategy, similar to what the TCP protocol uses (XXX reference?). Upon receiving a \textbf{preempted} message, a Leader will wait for some fixed period of time $t$ before restarting phase 1. If it is pre-empted again after restarting phase 1, it will wait for $2t$, and will continue to double the wait period until it completes phase 1 and becomes active. If a livelock situation arises, then because of the doubling timeout period, eventually some Leader will have enough time to complete phase 1 of the protocol and send out all proposals. This solution is simple, and behaves well with faults. Leaders are indefinite to node crashes; indeed this gives them a better chance of a proposal, since there are few competing Leaders active.

Another approach is to use heartbeat messages. When a Leader $L$ receives a \textbf{preempted} message, it starts pinging the other Leader $L'$ who had a higher ballot number. While this Leader responds timely to pings, $L$ remains passive and does not send messages. If $L'$ does not respond to a heartbeat message within a given period, then $L$ will begin phase 1 of the protocol. This is the approach we took in our implementation.

\section{Experiences}
war stories

\subsection{Understanding the Algorithm}

Paxos has a reputation for being difficult to implement. Despite this, many papers describe Paxos quite straightforwardly and simply. However, all presentations we could find simply outline a simple, impractical version of the protocol. Many authors are vague when describing optimizations to keep the memory usage practical and additions to the protocol to solve leader contention issues. While arguably outside the main thrust of the conceptual core of Paxos, these questions are vital to implementers, and it would be extremely helpful if some authors described them with richer detail and attention.

\subsection{Distributed Versus Local}
Our initial prototype of the Synod protocol ran on a single JVM. We abused this environment, treating actors as local objects we could communicate with through method calls and field accesses, rather than strictly performing all communication through message passing.

A related technical issue as we transitioned from a local to a distributed implementation was JVM serializability. In order to send objects in messages to actors at possibly remote JVMs, the runtime must serialize the object into a form which can be reconstituted on the remote JVM. Unfortunately, Actor objects in the Scala Actor library cannot be serialized, and our initial implementation sent actors in messages so the receiver could respond to the sender, and print out an informative debug message using the sender's fields. Altogether, changing the implementation so it could work in a distributed setting with RemoteActors involved a surprising amount of work. 

As we were both new to Scala, we wanted to get a prototype working as quickly as possible without spending too much time studying APIs, so after reading the minimum about the Actor library we set to work. Given how time-consuming the required changes were, I believe this was a mistake. We should have started from the very beginning using APIs which support a distributed setting.

\subsection{Underestimating the Difficulty of Concurrency}
Our initial prototype had one leader, fixed ahead of time. We added multiple leaders relatively late in the timeline of the project, and didn't test the initial multiple leader implementation very much. We only discovered after integrating our Paxos service with the GUI that there were timing bugs with our multiple leader implementation. Unfortunately, this issue was nondeterministic, with some test runs proving successful and others deadlocking. 


\section{Optimizations}

\subsection{Garbage Collection}

\subsection{Messages}

The current model of message passing is extremely inefficient. Clients broadcast their commands to all Replicas, which then broadcast to all Leaders. In our current implementation, this is quadratic in the number of nodes, as each node hosts both a replica and a server. 

Instead of broadcasting, the Client could simply send their request to a random Replica. If the Client hears back from the replica within a given period, then the Client knows the command has entered the system. Otherwise, it will chose another replica. Assuming a majority of Replicas are still operational, the Client will eventually pick a functioning Replica. Of course, it could avoid resending requests to Replicas it knows are unresponsive.

We can take a similar approach for Replicas. Instead of broadcasting to all Leaders, Replicas should only forward their request to their co-located leader. Given our failure model is at the node level, not the actor level, if a Replica is operational then so is its co-located Leader. After receiving the \textbf{request} message, the Leader will spawn Commanders to begin phase 2 if it is active, or forward this message to the leader it is currently pinging if it is inactive.

A final idea to reduce the communication bandwidth is to remove the pinging mechanism altogether in favour of the exponential back off approach. This change could work well with the above optimizations. Under this change, 

hmmm not so sure about this, the leader might be waiting for a long time? bandwidth vs latency tradeoff? what is the max wait time? time for a leader to complete all phases? what about multiple leaders?

\section{Conclusions}
what we achieved, what we failed, etc.


% The following two commands are all you need in the
% initial runs of your .tex file to
% produce the bibliography for the citations in your paper.
\bibliographystyle{abbrv}
\bibliography{sigproc}  % sigproc.bib is the name of the Bibliography in this case
% You must have a proper ".bib" file
%  and remember to run:
% latex bibtex latex latex
% to resolve all references
%
% ACM needs 'a single self-contained file'!
%
%APPENDICES are optional
%\balancecolumns
% That's all folks!
\end{document}
